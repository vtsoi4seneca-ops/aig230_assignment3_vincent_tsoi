{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91489c36",
   "metadata": {},
   "source": [
    "# AIG230 NLP (Week 3 Lab) — Notebook 2: Statistical Language Models (Train, Test, Evaluate)\n",
    "\n",
    "This notebook focuses on **n-gram Statistical Language Models (SLMs)**:\n",
    "- Train **unigram**, **bigram**, **trigram** models\n",
    "- Handle **OOV** with `<UNK>`\n",
    "- Apply **smoothing** (Add-k)\n",
    "- Evaluate with **cross-entropy** and **perplexity**\n",
    "- Do **next-word prediction** and simple **text generation**\n",
    "\n",
    "> Industry framing: even if modern systems use neural LMs, n-gram LMs are still useful for\n",
    "baselines, constrained domains, and for understanding evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94cb39",
   "metadata": {},
   "source": [
    "### What is smoothing?\n",
    "\n",
    "Smoothing is a way to stop a language model from saying “this can never happen.”\n",
    "\n",
    "When we train a language model from data, it only knows what it has seen before.\n",
    "If it never saw a particular word sequence, the model would normally give it a probability of zero.\n",
    "\n",
    "Smoothing fixes that.\n",
    "### Why is this a problem without smoothing?\n",
    "\n",
    "Imagine the model learned English only by reading a small number of news articles.\n",
    "\n",
    "If it never saw:\n",
    "\n",
    "- “oil prices explode”\n",
    "\n",
    "the model would conclude:\n",
    "\n",
    "- “That sentence is impossible.”\n",
    "\n",
    "But as humans, we know it could happen. The model just hasn’t seen it yet.\n",
    "\n",
    "Without smoothing:\n",
    "\n",
    "- One unseen word makes the whole sentence probability zero\n",
    "\n",
    "- Evaluation breaks\n",
    "\n",
    "- The model is too confident and too brittle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a046e",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ee526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc428b4",
   "metadata": {},
   "source": [
    "## 1) Data: domain text you might see in real systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d207e4c",
   "metadata": {},
   "source": [
    "We use short texts that resemble:\n",
    "- release notes\n",
    "- incident summaries\n",
    "- operational runbooks\n",
    "- customer support messaging\n",
    "\n",
    "In practice, you would load thousands to millions of lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb34582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 4,\n",
       " ['printer driver install fails with error 1603',\n",
       "  'push notifications not working on android app'],\n",
       " ['email delivery delayed messages queued',\n",
       "  'vpn disconnects frequently after windows update'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = [\n",
    "    \"vpn disconnects frequently after windows update\",\n",
    "    \"password reset link expired user cannot login\",\n",
    "    \"api requests timeout when latency spikes\",\n",
    "    \"portal returns 500 error after deployment\",\n",
    "    \"email delivery delayed messages queued\",\n",
    "    \"mfa prompt never arrives user stuck at login\",\n",
    "    \"wifi drops in meeting rooms access point reboot helps\",\n",
    "    \"outlook search not returning results index corrupted\",\n",
    "    \"printer driver install fails with error 1603\",\n",
    "    \"teams calls choppy audio jitter high\",\n",
    "    \"permission denied accessing shared drive though in correct group\",\n",
    "    \"battery drains fast after bios update power settings unchanged\",\n",
    "    \"push notifications not working on android app\",\n",
    "    \"mailbox full cannot receive emails auto archive not running\",\n",
    "]\n",
    "\n",
    "# Train/test split at sentence level\n",
    "random.seed(42)\n",
    "random.shuffle(corpus)\n",
    "split = int(0.75 * len(corpus))\n",
    "train_texts = corpus[:split]\n",
    "test_texts = corpus[split:]\n",
    "\n",
    "len(train_texts), len(test_texts), train_texts[:2], test_texts[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f9947",
   "metadata": {},
   "source": [
    "## 2) Tokenization + special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c285f1d",
   "metadata": {},
   "source": [
    "We will:\n",
    "- lowercase\n",
    "- keep alphanumerics\n",
    "- split on whitespace\n",
    "- add sentence boundary tokens: `<s>` and `</s>`\n",
    "\n",
    "We will also map rare tokens to `<UNK>` based on training frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058f87da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'printer',\n",
       " 'driver',\n",
       " 'install',\n",
       " 'fails',\n",
       " 'with',\n",
       " 'error',\n",
       " '1603',\n",
       " '</s>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "def add_boundaries(tokens: List[str], n: int) -> List[str]:\n",
    "    # For n-grams, prepend (n-1) start tokens for simpler context handling\n",
    "    return [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(\"Printer driver install fails with error 1603\")\n",
    "add_boundaries(tokens, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25308557",
   "metadata": {},
   "source": [
    "## 3) Build vocabulary and handle OOV with <UNK>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3338f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['email', 'delivery', 'delayed', 'messages', 'queued'],\n",
       " ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Build vocab from training data\n",
    "train_tokens_flat = []\n",
    "for t in train_texts:\n",
    "    train_tokens_flat.extend(tokenize(t))\n",
    "\n",
    "freq = Counter(train_tokens_flat)\n",
    "\n",
    "# Typical practical rule: map tokens with frequency <= 1 to <UNK> in small corpora\n",
    "min_count = 2\n",
    "vocab = {w for w, c in freq.items() if c >= min_count}\n",
    "vocab |= {\"<UNK>\", \"<s>\", \"</s>\"}\n",
    "\n",
    "def replace_oov(tokens: List[str], vocab: set) -> List[str]:\n",
    "    return [tok if tok in vocab else \"<UNK>\" for tok in tokens]\n",
    "\n",
    "# Show OOV effect\n",
    "sample = tokenize(test_texts[0])\n",
    "sample, replace_oov(sample, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759444a0",
   "metadata": {},
   "source": [
    "## 4) Train n-gram counts (unigram, bigram, trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eac8fa",
   "metadata": {},
   "source": [
    "We will compute:\n",
    "- `ngram_counts[(w1,...,wn)]`\n",
    "- `context_counts[(w1,...,w_{n-1})]`\n",
    "\n",
    "Then probability:\n",
    "\\ndefault:  P(w_n | context) = count(context + w_n) / count(context)\n",
    "\n",
    "This fails when an n-gram is unseen, so we add smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33672bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            \n",
    "def train_ngram_counts(texts: List[str], n: int, vocab: set) -> Dict[Tuple[str, ...], int]:\n",
    "    ngram_counts = Counter()\n",
    "    context_counts = Counter()\n",
    "    for text in texts:\n",
    "        toks = replace_oov(tokenize(text), vocab)\n",
    "        toks = add_boundaries(toks, n)\n",
    "        for ng in get_ngrams(toks, n):\n",
    "            ngram_counts[ng] += 1\n",
    "            context = ng[:-1]\n",
    "            context_counts[context] += 1\n",
    "            \n",
    "    return ngram_counts, context_counts       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6938ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_counts, uni_ctx = train_ngram_counts(train_texts, n=1, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5af370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>',): 67,\n",
       "         ('</s>',): 10,\n",
       "         ('not',): 3,\n",
       "         ('error',): 2,\n",
       "         ('after',): 2})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7f4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_counts, bi_ctx = train_ngram_counts(train_texts, n=2, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ff83df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>', '<UNK>'): 51,\n",
       "         ('<s>', '<UNK>'): 10,\n",
       "         ('<UNK>', '</s>'): 10,\n",
       "         ('<UNK>', 'not'): 3,\n",
       "         ('not', '<UNK>'): 3,\n",
       "         ('<UNK>', 'error'): 2,\n",
       "         ('after', '<UNK>'): 2,\n",
       "         ('error', '<UNK>'): 1,\n",
       "         ('<UNK>', 'after'): 1,\n",
       "         ('error', 'after'): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1e4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_counts, tri_ctx = train_ngram_counts(train_texts, n=3, vocab=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc689bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>', '<UNK>', '<UNK>'): 38,\n",
       "         ('<s>', '<s>', '<UNK>'): 10,\n",
       "         ('<s>', '<UNK>', '<UNK>'): 10,\n",
       "         ('<UNK>', '<UNK>', '</s>'): 7,\n",
       "         ('<UNK>', '<UNK>', 'not'): 3,\n",
       "         ('<UNK>', 'not', '<UNK>'): 3,\n",
       "         ('<UNK>', '<UNK>', 'error'): 2,\n",
       "         ('not', '<UNK>', '<UNK>'): 2,\n",
       "         ('<UNK>', 'error', '<UNK>'): 1,\n",
       "         ('error', '<UNK>', '</s>'): 1,\n",
       "         ('not', '<UNK>', '</s>'): 1,\n",
       "         ('<UNK>', '<UNK>', 'after'): 1,\n",
       "         ('<UNK>', 'after', '<UNK>'): 1,\n",
       "         ('after', '<UNK>', '<UNK>'): 1,\n",
       "         ('<UNK>', 'error', 'after'): 1,\n",
       "         ('error', 'after', '<UNK>'): 1,\n",
       "         ('after', '<UNK>', '</s>'): 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ba7c8",
   "metadata": {},
   "source": [
    "## 5) Add-k smoothing and probability function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed806986",
   "metadata": {},
   "source": [
    "### What does Add-k smoothing do?\n",
    "Add-k smoothing tells the model:\n",
    "\n",
    "- “Even if you didn’t see something, assume it could still happen a little bit.”\n",
    "\n",
    "It does this by:\n",
    "\n",
    "- Giving every possible next word a tiny amount of probability\n",
    "\n",
    "- Not just the ones seen in training\n",
    "\n",
    "So instead of:\n",
    "\n",
    "- seen → possible\n",
    "\n",
    "- unseen → impossible\n",
    "\n",
    "We get:\n",
    "\n",
    "- seen → more likely\n",
    "\n",
    "- unseen → less likely, but still possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea279f3",
   "metadata": {},
   "source": [
    "### Why is it called Add-k?\n",
    "\n",
    "Because we add a small number k to every word count.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "- adding a tiny “imaginary observation” for every word\n",
    "\n",
    "- so no word ever has zero probability\n",
    "\n",
    "When k is small (like 0.1 or 0.5), it gently smooths the probabilities instead of overpowering real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de565994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the probability of a word appearing next, given the previous words, while making sure the probability is never zero.\n",
    "def prob_addk(n_gram: Tuple[str, ...], ngram_counts: Counter, context_counts: Counter, V:int, k: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Compute add-k P(w_n | w_1 ... w_(n-1))\n",
    "    where ngram = (w_1, w_2, ..., w_n)\n",
    "    0 < k <= 1\n",
    "    V is the vocabulary size\n",
    "    \"\"\"\n",
    "    context = n_gram[:-1]\n",
    "    return (ngram_counts[n_gram] + k) / (context_counts[context] + k * V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf731d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038461538461538464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "example = (\"<s>\", \"login\")\n",
    "prob_addk(example, bi_counts, bi_ctx, V, k=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6deec",
   "metadata": {},
   "source": [
    "## 6) Evaluate: cross-entropy and perplexity on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8426d9",
   "metadata": {},
   "source": [
    "We evaluate an LM by how well it predicts held-out text.\n",
    "\n",
    "Cross-entropy (average negative log probability):\n",
    "H = - (1/N) * sum log2 P(w_i | context)\n",
    "\n",
    "Perplexity:\n",
    "PP = 2^H\n",
    "\n",
    "Lower perplexity is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d03099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(texts: List[str], n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, k: float = 0.5) -> float:\n",
    "    V = len(vocab)\n",
    "    log2_probs = []\n",
    "    token_count = 0\n",
    "\n",
    "    for text in texts:\n",
    "        toks = replace_oov(tokenize(text), vocab)\n",
    "        toks = add_boundaries(toks, n)\n",
    "        ngrams = get_ngrams(toks, n)\n",
    "        for ng in ngrams:\n",
    "            p = prob_addk(ng, ngram_counts, context_counts, V, k=k)\n",
    "            log2_probs.append(math.log(p, 2))\n",
    "            token_count += 1\n",
    "\n",
    "    H = -sum(log2_probs) / token_count\n",
    "    PP = 2 ** H\n",
    "    return PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3fce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8224739937573897, 1.8712095221558311, 1.9552746520172757)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_uni = evaluate_perplexity(test_texts, n=1, ngram_counts=uni_counts, context_counts=uni_ctx, vocab=vocab, k=0.5)\n",
    "pp_bi  = evaluate_perplexity(test_texts, n=2, ngram_counts=bi_counts,  context_counts=bi_ctx,  vocab=vocab, k=0.5)\n",
    "pp_tri = evaluate_perplexity(test_texts, n=3, ngram_counts=tri_counts, context_counts=tri_ctx, vocab=vocab, k=0.5)\n",
    "\n",
    "pp_uni, pp_bi, pp_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef338ce",
   "metadata": {},
   "source": [
    "## 7) Next-word prediction (top-k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d202f5",
   "metadata": {},
   "source": [
    "Given a context, compute the probability of each candidate next token and return the top-k.\n",
    "\n",
    "This mirrors:\n",
    "- autocomplete in constrained domains\n",
    "- template suggestion systems\n",
    "- command prediction in runbooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11363d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<UNK>', 0.8076923076923077),\n",
       " ('not', 0.038461538461538464),\n",
       " ('</s>', 0.038461538461538464),\n",
       " ('after', 0.038461538461538464),\n",
       " ('error', 0.038461538461538464)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def next_word_topk(context_tokens: List[str], n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, k_smooth: float = 0.5, top_k: int = 5):\n",
    "    # Context length should be n-1\n",
    "    V = len(vocab)\n",
    "    context = tuple(context_tokens[-(n-1):]) if n > 1 else tuple()\n",
    "    candidates = []\n",
    "    for w in vocab:\n",
    "        if w in {\"<s>\"}:\n",
    "            continue\n",
    "        ng = context + (w,)\n",
    "        p = prob_addk(ng, ngram_counts, context_counts, V, k=k_smooth)\n",
    "        candidates.append((w, p))\n",
    "    candidates.sort(key=lambda x: -x[1])\n",
    "    return candidates[:top_k]\n",
    "\n",
    "# Bigram: context is 1 token\n",
    "next_word_topk([\"<s>\"], n=2, ngram_counts=bi_counts, context_counts=bi_ctx, vocab=vocab, top_k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672e1e9",
   "metadata": {},
   "source": [
    "## 8) Simple generation (bigram or trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd41fb",
   "metadata": {},
   "source": [
    "Text generation is not the main goal in SLMs, but it helps you verify:\n",
    "- boundary handling\n",
    "- smoothing\n",
    "- OOV decisions\n",
    "\n",
    "We will sample tokens until we hit `</s>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c8d1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAM: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "BIGRAM: <UNK> <UNK> <UNK> <UNK> <UNK> not <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "BIGRAM: <UNK> <UNK> <UNK>\n",
      "BIGRAM: <UNK> <UNK> <UNK> <UNK> not error after after <UNK> <UNK>\n",
      "BIGRAM: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "def sample_next(context_tokens: List[str], n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, k_smooth: float = 0.5):\n",
    "    V = len(vocab)\n",
    "    context = tuple(context_tokens[-(n-1):]) if n > 1 else tuple()\n",
    "    words = [w for w in vocab if w != \"<s>\"]\n",
    "    probs = []\n",
    "    for w in words:\n",
    "        ng = context + (w,)\n",
    "        probs.append(prob_addk(ng, ngram_counts, context_counts, V, k=k_smooth))\n",
    "    # Normalize\n",
    "    s = sum(probs)\n",
    "    probs = [p/s for p in probs]\n",
    "    return random.choices(words, weights=probs, k=1)[0]\n",
    "\n",
    "def generate(n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, max_len: int = 20, k_smooth: float = 0.5):\n",
    "    tokens = [\"<s>\"]*(n-1) if n > 1 else []\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        w = sample_next(tokens, n, ngram_counts, context_counts, vocab, k_smooth=k_smooth)\n",
    "        if w == \"</s>\":\n",
    "            break\n",
    "        out.append(w)\n",
    "        tokens.append(w)\n",
    "    return \" \".join(out)\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"BIGRAM:\", generate(2, bi_counts, bi_ctx, vocab, max_len=18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db5405",
   "metadata": {},
   "source": [
    "## 9) Model comparison: effect of n and smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486afd",
   "metadata": {},
   "source": [
    "Try different `k` values. Notes:\n",
    "- `k=1.0` is Laplace smoothing (often too strong)\n",
    "- smaller `k` (like 0.1 to 0.5) is often better\n",
    "\n",
    "In real corpora, trigrams often beat bigrams, but require more data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb25609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1.0:  bigram PP=1.95   trigram PP=2.09\n",
      "k= 0.5:  bigram PP=1.87   trigram PP=1.96\n",
      "k= 0.1:  bigram PP=1.79   trigram PP=1.80\n",
      "k=0.01:  bigram PP=1.76   trigram PP=1.75\n"
     ]
    }
   ],
   "source": [
    "for k in [1.0, 0.5, 0.1, 0.01]:\n",
    "    pp_bi_k  = evaluate_perplexity(test_texts, n=2, ngram_counts=bi_counts,  context_counts=bi_ctx,  vocab=vocab, k=k)\n",
    "    pp_tri_k = evaluate_perplexity(test_texts, n=3, ngram_counts=tri_counts, context_counts=tri_ctx, vocab=vocab, k=k)\n",
    "    print(f\"k={k:>4}:  bigram PP={pp_bi_k:,.2f}   trigram PP={pp_tri_k:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be49abe",
   "metadata": {},
   "source": [
    "## Exercises (do these during lab)\n",
    "1) Add 20 more realistic domain sentences to the corpus and re-run training/evaluation.  \n",
    "2) Change `min_count` (OOV threshold) and explain how perplexity changes.  \n",
    "3) Implement **backoff**: if a trigram is unseen, fall back to bigram; if unseen, fall back to unigram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5c844",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d22be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 20 more realistic domain sentences to corpus\n",
    "additional_sentences = [\n",
    "    \"chrome browser crashes when opening multiple tabs memory leak detected\",\n",
    "    \"laptop overheating during video calls fan speed insufficient\",\n",
    "    \"external monitor not detected after docking station firmware update\",\n",
    "    \"onedrive sync stuck at processing changes local cache needs clearing\",\n",
    "    \"citrix session disconnects every fifteen minutes gateway timeout error\",\n",
    "    \"adobe acrobat freezes when printing to network printer spooler service\",\n",
    "    \"skype for business signs out randomly credential cache corrupted\",\n",
    "    \"vpn tunnel drops when switching from wifi to ethernet adapter conflict\",\n",
    "    \"sql server connection failed tcp ip protocol not enabled in configuration\",\n",
    "    \"git pull hangs indefinitely authentication token expired needs regeneration\",\n",
    "    \"jenkins build fails maven dependencies not resolving from repository\",\n",
    "    \"docker container exits immediately port already in use by another process\",\n",
    "    \"kubernetes pod stuck in pending state insufficient cluster resources\",\n",
    "    \"splunk dashboard shows no data forwarder misconfigured on host\",\n",
    "    \"ssl certificate expired causing browser security warnings for users\",\n",
    "    \"active directory replication failing between domain controllers dns issue\",\n",
    "    \"sharepoint document library not loading list view threshold exceeded\",\n",
    "    \"zoom screen share displays black window graphics driver outdated\",\n",
    "    \"backup job failed destination storage volume ran out of disk space\",\n",
    "    \"patch management server offline endpoints not receiving security updates\",\n",
    "]\n",
    "\n",
    "# Extend original corpus and re-shuffle\n",
    "corpus_extended = corpus + additional_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550e559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended corpus: 34 sentences\n",
      "Train: 25, Test: 9\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(corpus_extended)\n",
    "split = int(0.75 * len(corpus_extended))\n",
    "train_texts_extended = corpus_extended[:split]\n",
    "test_texts_extended = corpus_extended[split:]\n",
    "\n",
    "print(f\"Extended corpus: {len(corpus_extended)} sentences\")\n",
    "print(f\"Train: {len(train_texts_extended)}, Test: {len(test_texts_extended)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d8dda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with extended corpus (min_count=2 as baseline)\n",
    "train_tokens_flat_ext = []\n",
    "for t in train_texts_extended:\n",
    "    train_tokens_flat_ext.extend(tokenize(t))\n",
    "freq_ext = Counter(train_tokens_flat_ext)\n",
    "\n",
    "min_count_baseline = 2\n",
    "vocab_extended = {w for w, c in freq_ext.items() if c >= min_count_baseline}\n",
    "vocab_extended |= {\"<UNK>\", \"<s>\", \"</s>\"}\n",
    "V_ext = len(vocab_extended)\n",
    "\n",
    "# Retrain n-gram models\n",
    "uni_counts_ext, uni_ctx_ext = train_ngram_counts(train_texts_extended, n=1, vocab=vocab_extended)\n",
    "bi_counts_ext, bi_ctx_ext = train_ngram_counts(train_texts_extended, n=2, vocab=vocab_extended)\n",
    "tri_counts_ext, tri_ctx_ext = train_ngram_counts(train_texts_extended, n=3, vocab=vocab_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f520e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extended Corpus Results (min_count=2):\n",
      "  Unigram PP: 2.78\n",
      "  Bigram PP:  3.14\n",
      "  Trigram PP: 3.75\n",
      "\n",
      "Comparison with Original (14 sentences) vs Extended (34 sentences):\n",
      "  Original Trigram PP: 1.96\n",
      "  Extended Trigram PP: 3.75\n"
     ]
    }
   ],
   "source": [
    "# Evaluate extended models\n",
    "pp_uni_ext = evaluate_perplexity(test_texts_extended, n=1, ngram_counts=uni_counts_ext, \n",
    "                                  context_counts=uni_ctx_ext, vocab=vocab_extended, k=0.5)\n",
    "pp_bi_ext = evaluate_perplexity(test_texts_extended, n=2, ngram_counts=bi_counts_ext, \n",
    "                                 context_counts=bi_ctx_ext, vocab=vocab_extended, k=0.5)\n",
    "pp_tri_ext = evaluate_perplexity(test_texts_extended, n=3, ngram_counts=tri_counts_ext, \n",
    "                                  context_counts=tri_ctx_ext, vocab=vocab_extended, k=0.5)\n",
    "\n",
    "print(f\"\\nExtended Corpus Results (min_count={min_count_baseline}):\")\n",
    "print(f\"  Unigram PP: {pp_uni_ext:.2f}\")\n",
    "print(f\"  Bigram PP:  {pp_bi_ext:.2f}\")\n",
    "print(f\"  Trigram PP: {pp_tri_ext:.2f}\")\n",
    "\n",
    "# Compare with original\n",
    "print(f\"\\nComparison with Original (14 sentences) vs Extended (34 sentences):\")\n",
    "print(f\"  Original Trigram PP: {pp_tri:.2f}\")\n",
    "print(f\"  Extended Trigram PP: {pp_tri_ext:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f11202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Effect of min_count (OOV threshold) on Perplexity\n",
      "============================================================\n",
      "\n",
      "min_count = 1:\n",
      "  Vocab size: 188 (+ 3 special tokens)\n",
      "  OOV rate in test: 69.9%\n",
      "  Unigram PP: 307.40\n",
      "  Bigram PP:  191.60\n",
      "  Trigram PP: 195.94\n",
      "\n",
      "min_count = 2:\n",
      "  Vocab size: 21 (+ 3 special tokens)\n",
      "  OOV rate in test: 89.0%\n",
      "  Unigram PP:   2.78\n",
      "  Bigram PP:    3.14\n",
      "  Trigram PP:   3.75\n",
      "\n",
      "min_count = 3:\n",
      "  Vocab size: 4 (+ 3 special tokens)\n",
      "  OOV rate in test: 98.6%\n",
      "  Unigram PP:   1.60\n",
      "  Bigram PP:    1.60\n",
      "  Trigram PP:   1.58\n",
      "\n",
      "min_count = 4:\n",
      "  Vocab size: 2 (+ 3 special tokens)\n",
      "  OOV rate in test: 98.6%\n",
      "  Unigram PP:   1.55\n",
      "  Bigram PP:    1.54\n",
      "  Trigram PP:   1.53\n"
     ]
    }
   ],
   "source": [
    "# Change min_count and explain perplexity changes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Effect of min_count (OOV threshold) on Perplexity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for min_count in [1, 2, 3, 4]:\n",
    "    # Build vocab with different threshold\n",
    "    vocab_test = {w for w, c in freq_ext.items() if c >= min_count}\n",
    "    vocab_test |= {\"<UNK>\", \"<s>\", \"</s>\"}\n",
    "    V_test = len(vocab_test)\n",
    "    \n",
    "    # Retrain all n-grams with new vocab\n",
    "    uni_c, uni_ctx_c = train_ngram_counts(train_texts_extended, n=1, vocab=vocab_test)\n",
    "    bi_c, bi_ctx_c = train_ngram_counts(train_texts_extended, n=2, vocab=vocab_test)\n",
    "    tri_c, tri_ctx_c = train_ngram_counts(train_texts_extended, n=3, vocab=vocab_test)\n",
    "    # Evaluate\n",
    "    pp_u = evaluate_perplexity(test_texts_extended, n=1, ngram_counts=uni_c, \n",
    "                               context_counts=uni_ctx_c, vocab=vocab_test, k=0.5)\n",
    "    pp_b = evaluate_perplexity(test_texts_extended, n=2, ngram_counts=bi_c, \n",
    "                               context_counts=bi_ctx_c, vocab=vocab_test, k=0.5)\n",
    "    pp_t = evaluate_perplexity(test_texts_extended, n=3, ngram_counts=tri_c, \n",
    "                               context_counts=tri_ctx_c, vocab=vocab_test, k=0.5)\n",
    "    # Count OOV occurrences in test set\n",
    "    test_tokens_flat = []\n",
    "    for t in test_texts_extended:\n",
    "        test_tokens_flat.extend(tokenize(t))\n",
    "    oov_count = sum(1 for tok in test_tokens_flat if tok not in vocab_test and tok != \"<UNK>\")\n",
    "    oov_rate = oov_count / len(test_tokens_flat) * 100\n",
    "    \n",
    "    print(f\"\\nmin_count = {min_count}:\")\n",
    "    print(f\"  Vocab size: {V_test-3} (+ 3 special tokens)\")\n",
    "    print(f\"  OOV rate in test: {oov_rate:.1f}%\")\n",
    "    print(f\"  Unigram PP: {pp_u:6.2f}\")\n",
    "    print(f\"  Bigram PP:  {pp_b:6.2f}\")\n",
    "    print(f\"  Trigram PP: {pp_t:6.2f}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a466ad",
   "metadata": {},
   "source": [
    "Explanation of min_count effects:\n",
    "- min_count=1: No OOV mapping. High sparsity, many n-grams with zero counts.\n",
    "  Perplexity tends to be high due to unreliable probability estimates for rare words.\n",
    "  \n",
    "- min_count=2: Maps hapax legomena (single-occurrence words) to <UNK>.\n",
    "  Usually OPTIMAL for small corpora - balances vocabulary coverage vs. sparsity.\n",
    "  <UNK> token absorbs probability mass of rare events more reliably than singletons.\n",
    "  \n",
    "- min_count=3: Aggressive OOV filtering. Loses valid technical terms that appear twice.\n",
    "  May increase perplexity if test set contains many moderately rare but valid terms\n",
    "  that get mapped to <UNK>, reducing prediction specificity.\n",
    "  \n",
    "- min_count=4+: Too aggressive for small domain corpora. Vocabulary becomes too small,\n",
    "  model underfits, unable to distinguish between important domain concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Backoff Smoothing Implementation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Implement Backoff (trigram -> bigram -> unigram)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Backoff Smoothing Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def prob_with_backoff(n_gram: Tuple[str, ...], \n",
    "                     ngram_counts: Counter, context_counts: Counter,\n",
    "                     backoff_counts: Counter, backoff_context: Counter,\n",
    "                     unigram_counts: Counter, V: int, \n",
    "                     alpha: float = 0.4) -> float:\n",
    "    \"\"\"\n",
    "    if n-gram unseen, back off to (n-1)-gram.\n",
    "    For trigram -> bigram -> unigram.\n",
    "    Unigrams use add-k smoothing to avoid zero.\n",
    "    \"\"\"\n",
    "    n = len(n_gram)\n",
    "    context = n_gram[:-1]\n",
    "    \n",
    "    if n == 3:\n",
    "        # Try trigram first\n",
    "        if ngram_counts[n_gram] > 0:\n",
    "            return ngram_counts[n_gram] / context_counts[context]\n",
    "        else:\n",
    "            # Backoff to bigram with discount\n",
    "            backoff_ngram = n_gram[1:]  # (w2, w3)\n",
    "            return alpha * prob_with_backoff(backoff_ngram,\n",
    "                                           backoff_counts, backoff_context,\n",
    "                                           unigram_counts, None,\n",
    "                                           unigram_counts, V, alpha)\n",
    "    \n",
    "    elif n == 2:\n",
    "        # Try bigram\n",
    "        if ngram_counts[n_gram] > 0:\n",
    "            return ngram_counts[n_gram] / context_counts[context]\n",
    "        else:\n",
    "            # Backoff to unigram with discount\n",
    "            word = n_gram[-1]\n",
    "            k = 0.01  # Small add-k for unigram\n",
    "            total_unigrams = sum(unigram_counts.values())\n",
    "            return alpha * (unigram_counts[(word,)] + k) / (total_unigrams + k * V)\n",
    "    \n",
    "    else:\n",
    "        # Unigram base case (shouldn't reach here in recursive calls)\n",
    "        word = n_gram[-1]\n",
    "        k = 0.01\n",
    "        total_unigrams = sum(unigram_counts.values())\n",
    "        return (unigram_counts[(word,)] + k) / (total_unigrams + k * V)\n",
    "\n",
    "def evaluate_perplexity_backoff(test_texts: List[str], \n",
    "                                tri_counts: Counter, tri_ctx: Counter,\n",
    "                                bi_counts: Counter, bi_ctx: Counter,\n",
    "                                uni_counts: Counter, vocab: set, \n",
    "                                alpha: float = 0.4) -> float:\n",
    "    \"\"\"Evaluate trigram model with backoff to bigram/unigram\"\"\"\n",
    "    V = len(vocab)\n",
    "    log2_probs = []\n",
    "    token_count = 0\n",
    "    \n",
    "    for text in test_texts:\n",
    "        toks = replace_oov(tokenize(text), vocab)\n",
    "        toks = add_boundaries(toks, n=3)\n",
    "        ngrams = get_ngrams(toks, 3)\n",
    "        \n",
    "        for ng in ngrams:\n",
    "            p = prob_with_backoff(ng, tri_counts, tri_ctx,\n",
    "                                 bi_counts, bi_ctx,\n",
    "                                 uni_counts, V, alpha)\n",
    "            # Safety check\n",
    "            if p <= 0:\n",
    "                p = 1e-10\n",
    "            log2_probs.append(math.log(p, 2))\n",
    "            token_count += 1\n",
    "    \n",
    "    H = -sum(log2_probs) / token_count\n",
    "    return 2 ** H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cc7f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Backoff vs Add-k Smoothing (Trigram):\n",
      "  Backoff (alpha=0.3): PP = 2.66\n",
      "  Backoff (alpha=0.4): PP = 2.61\n",
      "  Backoff (alpha=0.5): PP = 2.57\n",
      "  Add-k (k=0.5):            PP = 3.75\n"
     ]
    }
   ],
   "source": [
    "# Compare backoff vs add-k\n",
    "print(\"\\nComparing Backoff vs Add-k Smoothing (Trigram):\")\n",
    "for alpha in [0.3, 0.4, 0.5]:\n",
    "    pp_backoff = evaluate_perplexity_backoff(test_texts_extended,\n",
    "                                            tri_counts_ext, tri_ctx_ext,\n",
    "                                            bi_counts_ext, bi_ctx_ext,\n",
    "                                            uni_counts_ext, vocab_extended, alpha)\n",
    "    print(f\"  Backoff (alpha={alpha}): PP = {pp_backoff:.2f}\")\n",
    "\n",
    "pp_addk = evaluate_perplexity(test_texts_extended, n=3, ngram_counts=tri_counts_ext,\n",
    "                             context_counts=tri_ctx_ext, vocab=vocab_extended, k=0.5)\n",
    "print(f\"  Add-k (k=0.5):            PP = {pp_addk:.2f}\")\n",
    "\n",
    "# Demonstrate backoff prediction\n",
    "def next_word_backoff(context_tokens: List[str], \n",
    "                     tri_counts: Counter, tri_ctx: Counter,\n",
    "                     bi_counts: Counter, bi_ctx: Counter,\n",
    "                     uni_counts: Counter, vocab: set, \n",
    "                     alpha: float = 0.4, top_k: int = 5):\n",
    "    \"\"\"Predict next word using backoff from trigram to unigram\"\"\"\n",
    "    V = len(vocab)\n",
    "    context = tuple(context_tokens[-2:])  # Last 2 for trigram context\n",
    "    candidates = []\n",
    "    \n",
    "    for w in vocab:\n",
    "        if w in {\"<s>\"}:\n",
    "            continue\n",
    "        ng = context + (w,)\n",
    "        p = prob_with_backoff(ng, tri_counts, tri_ctx,\n",
    "                             bi_counts, bi_ctx,\n",
    "                             uni_counts, V, alpha)\n",
    "        candidates.append((w, p))\n",
    "    \n",
    "    candidates.sort(key=lambda x: -x[1])\n",
    "    return candidates[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "018c347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backoff Prediction Examples:\n",
      "\n",
      "Context: '<s> vpn'\n",
      "  <UNK>           0.500000 (trigram)\n",
      "  disconnects     0.500000 (trigram)\n",
      "  </s>            0.016451 (unigram)\n",
      "  not             0.004611 (unigram)\n",
      "  after           0.002638 (unigram)\n",
      "\n",
      "Context: 'password reset'\n",
      "  <UNK>           0.109857 (unigram)\n",
      "  </s>            0.016451 (unigram)\n",
      "  not             0.004611 (unigram)\n",
      "  after           0.002638 (unigram)\n",
      "  error           0.001980 (unigram)\n",
      "\n",
      "Context: 'email delivery'\n",
      "  <UNK>           0.109857 (unigram)\n",
      "  </s>            0.016451 (unigram)\n",
      "  not             0.004611 (unigram)\n",
      "  after           0.002638 (unigram)\n",
      "  error           0.001980 (unigram)\n"
     ]
    }
   ],
   "source": [
    "# Test backoff prediction\n",
    "print(\"\\nBackoff Prediction Examples:\")\n",
    "test_contexts = [\n",
    "    [\"<s>\", \"vpn\"],\n",
    "    [\"password\", \"reset\"],\n",
    "    [\"email\", \"delivery\"]\n",
    "]\n",
    "\n",
    "for ctx in test_contexts:\n",
    "    preds = next_word_backoff(ctx, tri_counts_ext, tri_ctx_ext,\n",
    "                             bi_counts_ext, bi_ctx_ext,\n",
    "                             uni_counts_ext, vocab_extended, alpha=0.4)\n",
    "    print(f\"\\nContext: '{' '.join(ctx)}'\")\n",
    "    for word, prob in preds:\n",
    "        status = \"\"\n",
    "        tri_check = tuple(ctx[-2:] + [word]) if len(ctx) >= 2 else tuple()\n",
    "        bi_check = tuple(ctx[-1:] + [word]) if len(ctx) >= 1 else tuple()\n",
    "        \n",
    "        if tri_check in tri_counts_ext and tri_counts_ext[tri_check] > 0:\n",
    "            status = \"(trigram)\"\n",
    "        elif bi_check in bi_counts_ext and bi_counts_ext[bi_check] > 0:\n",
    "            status = \"(bigram)\"\n",
    "        else:\n",
    "            status = \"(unigram)\"\n",
    "            \n",
    "        print(f\"  {word:15} {prob:.6f} {status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46d4e4",
   "metadata": {},
   "source": [
    "Backoff vs Add-k:\n",
    "- Backoff preserves maximum likelihood estimates for seen n-grams (no smoothing)\n",
    "- Only applies smoothing/discount when backing off to lower orders\n",
    "- Usually achieves lower perplexity than fixed add-k smoothing\n",
    "- Alpha (discount factor) controls how much probability mass to reserve for unseen events\n",
    "  - Lower alpha (0.3): less mass for unseen, more for seen events\n",
    "  - Higher alpha (0.5): more mass for unseen events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIG230)",
   "language": "python",
   "name": "aig230-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
